{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T06:31:45.393604Z",
     "start_time": "2021-05-11T06:31:44.999165Z"
    }
   },
   "outputs": [],
   "source": [
    "def search_weighted_word_embedding(query):\n",
    "    '''\n",
    "    The function uses weighted word embedding to process documents, \n",
    "    and print the best outfit we can find in product and outfit files.\n",
    "    \n",
    "    '''\n",
    "    # Input packages and dataset\n",
    "    import pandas as pd\n",
    "    import spacy\n",
    "    import en_core_web_lg\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    import numpy as np\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    df = pd.read_csv('processed_product.csv')\n",
    "    out_fit = pd.read_csv('outfit_combinations.csv')\n",
    "    \n",
    "    # Output is expected to be a list of dictionaries.\n",
    "    output = []\n",
    "    \n",
    "    # check if the query is a product ID\n",
    "    # If not a product ID, we would do doc2vec after.\n",
    "    # If it is a product ID, print the product name.\n",
    "    # Moreover, if the product ID is in human domain experts combos, print the combinations.        \n",
    "    out_fit_products = list(out_fit['product_id'].unique())\n",
    "    # check if the query is the product ID\n",
    "    if query in list(df['product_id']):\n",
    "        matched_product_name = df[df['product_id']==query]['name'].values[0]\n",
    "        print(f'Matched Product: {matched_product_name} ({query})\\n')\n",
    "        if query in out_fit_products:\n",
    "            print('WOW! The product is in a great combination(s):\\n')\n",
    "            matched_outfits = out_fit[out_fit['product_id']==query]['outfit_id'].unique()\n",
    "            for outfit in matched_outfits:\n",
    "                outfit_details = out_fit[out_fit['outfit_id']==outfit].reset_index()\n",
    "                output.append(dict(zip(outfit_details['outfit_item_type'],outfit_details['product_full_name']+'('+outfit_details['product_id']+')')))\n",
    "            combo_idx=1\n",
    "            for c in output:\n",
    "                print(f'Combo {combo_idx}:\\n')\n",
    "                combo_idx+=1\n",
    "                for i in c:\n",
    "                    print(f'{i}: {c[i]}\\n')\n",
    "        return None  \n",
    "\n",
    "    \n",
    "    # load spacy en_core_web_lg model\n",
    "    nlp = en_core_web_lg.load()\n",
    "    \n",
    "    # build tf-idf vector by name\n",
    "    vectorizer_name = TfidfVectorizer(ngram_range=(1, 1), \n",
    "                             stop_words=\"english\", \n",
    "                             max_features=1000,token_pattern='(?u)\\\\b[a-zA-Z][a-zA-Z]+\\\\b')\n",
    "    X = vectorizer_name.fit_transform(df['name'])\n",
    "\n",
    "    tf_idf_lookup_table = pd.DataFrame(X.toarray(), columns=vectorizer_name.get_feature_names())\n",
    "\n",
    "    DOCUMENT_SUM_COLUMN = \"DOCUMENT_TF_IDF_SUM\"\n",
    "\n",
    "    # sum the tf idf scores for each document\n",
    "    tf_idf_lookup_table[DOCUMENT_SUM_COLUMN] = tf_idf_lookup_table.sum(axis=1)\n",
    "    available_tf_idf_scores = tf_idf_lookup_table.columns # a list of all the columns we have\n",
    "\n",
    "\n",
    "    names_vectors = []\n",
    "    for idx, name in enumerate(df['name']): # iterate through each review\n",
    "        tokens = nlp(name) # have spacy tokenize the review text\n",
    "    \n",
    "    # initially start a running total of tf-idf scores for a document\n",
    "        total_tf_idf_score_per_document = 0\n",
    "    \n",
    "    # start a running total of initially all zeroes (300 is picked since that is the word embedding size used by word2vec)\n",
    "        running_total_word_embedding = np.zeros(300) \n",
    "        for token in tokens: # iterate through each token\n",
    "    \n",
    "    # if the token has a pretrained word embedding it also has a tf-idf score\n",
    "            if token.has_vector and token.text.lower() in available_tf_idf_scores:\n",
    "            \n",
    "                tf_idf_score = tf_idf_lookup_table.loc[idx, token.text.lower()]\n",
    "            #print(f\"{token} has tf-idf score of {tf_idf_lookup_table.loc[idx, token.text.lower()]}\")\n",
    "                running_total_word_embedding += tf_idf_score * token.vector\n",
    "            \n",
    "                total_tf_idf_score_per_document += tf_idf_score\n",
    "    \n",
    "    # divide the total embedding by the total tf-idf score for each document\n",
    "        document_embedding = running_total_word_embedding / total_tf_idf_score_per_document\n",
    "        names_vectors.append(document_embedding)\n",
    "\n",
    "        \n",
    "    # create tf-idf weighted word embedding vector for query (name model)\n",
    "    Q = vectorizer_name.transform(pd.Series(query))\n",
    "\n",
    "    tf_idf_lookup_table_q = pd.DataFrame(Q.toarray(), columns=vectorizer_name.get_feature_names())\n",
    "    QUERY_SUM_COLUMN = \"QUERY_TF_IDF_SUM\"\n",
    "\n",
    "    tf_idf_lookup_table_q[QUERY_SUM_COLUMN] = tf_idf_lookup_table_q.sum(axis=1)\n",
    "    available_tf_idf_scores = tf_idf_lookup_table_q.columns # a list of all the columns we have\n",
    "\n",
    "\n",
    "    q_tokens = nlp(query)\n",
    "\n",
    "    total_tf_idf_score_q = 0\n",
    "\n",
    "    running_total_word_embedding = np.zeros(300) \n",
    "\n",
    "    for token in q_tokens: # iterate through each token    \n",
    "        # if the token has a pretrained word embedding it also has a tf-idf score\n",
    "        if token.has_vector and token.text.lower() in available_tf_idf_scores:\n",
    "            tf_idf_score = tf_idf_lookup_table_q.loc[0, token.text.lower()]\n",
    "            #print(f\"{token} has tf-idf score of {tf_idf_lookup_table_q.loc[0, token.text.lower()]}\")\n",
    "            running_total_word_embedding += tf_idf_score * token.vector\n",
    "            total_tf_idf_score_q += tf_idf_score\n",
    "\n",
    "    q_embedding_name = running_total_word_embedding / total_tf_idf_score_q\n",
    "    \n",
    "    \n",
    "    # build tf-idf vector by description\n",
    "    vectorizer_desc = TfidfVectorizer(ngram_range=(1, 1), \n",
    "                             stop_words=\"english\", \n",
    "                             max_features=500,token_pattern='(?u)\\\\b[a-zA-Z][a-zA-Z]+\\\\b')\n",
    "    X = vectorizer_desc.fit_transform(df['description'])\n",
    "\n",
    "    tf_idf_lookup_table = pd.DataFrame(X.toarray(), columns=vectorizer_desc.get_feature_names())\n",
    "\n",
    "    DOCUMENT_SUM_COLUMN = \"DOCUMENT_TF_IDF_SUM\"\n",
    "\n",
    "    # sum the tf idf scores for each document\n",
    "    tf_idf_lookup_table[DOCUMENT_SUM_COLUMN] = tf_idf_lookup_table.sum(axis=1)\n",
    "    available_tf_idf_scores = tf_idf_lookup_table.columns # a list of all the columns we have\n",
    "\n",
    "\n",
    "    desc_vectors = []\n",
    "    for idx, desc in enumerate(df['description']): # iterate through each review\n",
    "        tokens = nlp(desc) # have spacy tokenize the review text\n",
    "    \n",
    "    # initially start a running total of tf-idf scores for a document\n",
    "        total_tf_idf_score_per_document = 0\n",
    "    \n",
    "    # start a running total of initially all zeroes (300 is picked since that is the word embedding size used by word2vec)\n",
    "        running_total_word_embedding = np.zeros(300) \n",
    "        for token in tokens: # iterate through each token\n",
    "    \n",
    "    # if the token has a pretrained word embedding it also has a tf-idf score\n",
    "            if token.has_vector and token.text.lower() in available_tf_idf_scores:\n",
    "            \n",
    "                tf_idf_score = tf_idf_lookup_table.loc[idx, token.text.lower()]\n",
    "            #print(f\"{token} has tf-idf score of {tf_idf_lookup_table.loc[idx, token.text.lower()]}\")\n",
    "                running_total_word_embedding += tf_idf_score * token.vector\n",
    "            \n",
    "                total_tf_idf_score_per_document += tf_idf_score\n",
    "    \n",
    "    # divide the total embedding by the total tf-idf score for each document\n",
    "        document_embedding = running_total_word_embedding / total_tf_idf_score_per_document\n",
    "        desc_vectors.append(document_embedding)\n",
    "\n",
    "        \n",
    "    # create query vector by description model\n",
    "    Q2 = vectorizer_desc.transform(pd.Series(query))\n",
    "\n",
    "    tf_idf_lookup_table_q2 = pd.DataFrame(Q2.toarray(), columns=vectorizer_desc.get_feature_names())\n",
    "    QUERY_SUM_COLUMN = \"QUERY_TF_IDF_SUM\"\n",
    "\n",
    "    tf_idf_lookup_table_q2[QUERY_SUM_COLUMN] = tf_idf_lookup_table_q2.sum(axis=1)\n",
    "    available_tf_idf_scores = tf_idf_lookup_table_q2.columns # a list of all the columns we have\n",
    "\n",
    "\n",
    "    q_tokens = nlp(query)\n",
    "\n",
    "    total_tf_idf_score_q = 0\n",
    "\n",
    "    running_total_word_embedding = np.zeros(300) \n",
    "\n",
    "    for token in q_tokens: # iterate through each token    \n",
    "        # if the token has a pretrained word embedding it also has a tf-idf score\n",
    "        if token.has_vector and token.text.lower() in available_tf_idf_scores:\n",
    "            tf_idf_score = tf_idf_lookup_table_q2.loc[0, token.text.lower()]\n",
    "            #print(f\"{token} has tf-idf score of {tf_idf_lookup_table_q.loc[0, token.text.lower()]}\")\n",
    "            running_total_word_embedding += tf_idf_score * token.vector\n",
    "            total_tf_idf_score_q += tf_idf_score\n",
    "\n",
    "    q_embedding_desc = running_total_word_embedding / total_tf_idf_score_q    \n",
    "    \n",
    "    # combine 2 vector together\n",
    "    # during trails, we find there are lots of noisy words in description\n",
    "    # thus, we set different weights for name column and description column\n",
    "    q_embedding_name[np.isnan(q_embedding_name)]=0\n",
    "    q_embedding_desc[np.isnan(q_embedding_desc)]=0    \n",
    "    q_embedding = np.append(0.8*q_embedding_name,0.2*q_embedding_desc)\n",
    "    \n",
    "    similarities = []\n",
    "    for i in range(len(names_vectors)):\n",
    "        a = names_vectors[i]\n",
    "        a[np.isnan(a)] = 0\n",
    "        b = desc_vectors[i]\n",
    "        b[np.isnan(b)] = 0\n",
    "        c = np.append(0.8*a, 0.2*b)\n",
    "        similarities.append(cosine_similarity(c.reshape(1,-1),q_embedding.reshape(1,-1))[0][0])\n",
    "    \n",
    "    # build similarity dataframe\n",
    "    similarities = pd.DataFrame(similarities,index=df['product_id'],columns=['similarity']).sort_values(by='similarity',ascending=False).reset_index()\n",
    "    similarities = pd.merge(similarities, df[['product_id','name','description','product_category']],on='product_id',how='left')\n",
    "    \n",
    "    \n",
    "    # find the most similar products \n",
    "    most_matched_product = similarities.loc[0,'product_id']\n",
    "    most_matched_product_name = similarities.loc[0,'name']\n",
    "\n",
    "    # threshold for 'good' similarity\n",
    "    if similarities.loc[0,'similarity'] <=0.4 or similarities.loc[0,'product_category']=='UNKNOWN_TOKEN':\n",
    "        print('Sorry, we do not find matched product. Please check if you are searching for clothing.')\n",
    "        return None\n",
    "     \n",
    "\n",
    "    # If the most similar product in outfit dataset, print out experts recommended combos\n",
    "    if most_matched_product in out_fit_products:\n",
    "        matched_outfits = out_fit[out_fit['product_id']==most_matched_product]['outfit_id'].unique()\n",
    "        for outfit in matched_outfits:\n",
    "            outfit_details = out_fit[out_fit['outfit_id']==outfit].reset_index()\n",
    "            output.append(dict(zip(outfit_details['outfit_item_type'],outfit_details['product_full_name']+'('+outfit_details['product_id']+')')))\n",
    "        print(f'The most similar product is {most_matched_product_name} ({most_matched_product})\\n')\n",
    "        print(\"WOW, the product is in human domain experts' outfit combination(s):\\n\")\n",
    "        combo_idx = 1\n",
    "        for c in output:\n",
    "            print(f'Combo {combo_idx}:\\n')\n",
    "            combo_idx+=1\n",
    "            for i in c:\n",
    "                print(f'{i}: {c[i]}\\n')    \n",
    "    # Otherwise, we give a recommendation by ourselves\n",
    "    # We have two format of combo: [top, bottom, shoes, accessory] or [one-piece, shoes, accessory]            \n",
    "    else:     \n",
    "        top = similarities[similarities['product_category']=='top'].iloc[0]['name']+'('+similarities[similarities['product_category']=='top'].iloc[0]['product_id']+')'        \n",
    "        bottom = similarities[similarities['product_category']=='bottom'].iloc[0]['name']+'('+similarities[similarities['product_category']=='bottom'].iloc[0]['product_id']+')'\n",
    "        onepiece = similarities[similarities['product_category']=='onepiece'].iloc[0]['name']+'('+similarities[similarities['product_category']=='onepiece'].iloc[0]['product_id']+')'\n",
    "        shoes = similarities[similarities['product_category']=='shoe'].iloc[0]['name']+'('+similarities[similarities['product_category']=='shoe'].iloc[0]['product_id']+')'\n",
    "        accessory = similarities[similarities['product_category']=='accessory'].iloc[0]['name']+'('+similarities[similarities['product_category']=='accessory'].iloc[0]['product_id']+')'\n",
    "        # if matched product is top/bottom, we recommend a combo with top, bottom, shoe, accessory        \n",
    "        if similarities[similarities['product_id']==most_matched_product]['product_category'].values[0] in ['top','bottom']:\n",
    "            output.append({'top':top,'bottom':bottom,'shoe':shoes,'accessory':accessory})\n",
    "        # if matched product is onepiece, we recommend a combo with onepiece, shoe, accessory            \n",
    "        elif similarities[similarities['product_id']==most_matched_product]['product_category'].values[0]=='onepiece':\n",
    "            output.append({'onepiece':onepiece,'shoe':shoes,'accessory':accessory})\n",
    "        # if matched product is shoe/accessory, we recommend 2 kinds of combos\n",
    "        else:\n",
    "            output.append({'top':top,'bottom':bottom,'shoe':shoes,'accessory':accessory})\n",
    "            output.append({'onepiece':onepiece,'shoe':shoes,'accessory':accessory})\n",
    "        print(f'The most similar product is {most_matched_product_name} ({most_matched_product})\\n')\n",
    "        print('Following are our recommended outfit combinations:\\n')\n",
    "        combo_idx = 1\n",
    "        for c in output:\n",
    "            print(f'Combo {combo_idx}:\\n')\n",
    "            combo_idx+=1\n",
    "            for i in c:\n",
    "                print(f'{i}: {c[i]}\\n')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T07:10:08.484225Z",
     "start_time": "2021-05-11T06:51:41.660235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, we do not find matched product. Please check if you are searching for clothing.\n"
     ]
    }
   ],
   "source": [
    "search_weighted_word_embedding('MacBook computer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T06:50:44.445059Z",
     "start_time": "2021-05-11T06:50:43.500638Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched Product: clara (01EWTHFH4H3GP0Q34E6JBYJZNZ)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_weighted_word_embedding('01EWTHFH4H3GP0Q34E6JBYJZNZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T06:50:58.460852Z",
     "start_time": "2021-05-11T06:50:57.517874Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched Product: juan embossed mules (01DVA59VHYAPT4PVX32NXW91G5)\n",
      "\n",
      "WOW! The product is in a great combination(s):\n",
      "\n",
      "Combo 1:\n",
      "\n",
      "top: Knightley Striped Cotton-Voile Shirt(01DTATDR81EZ9S7DTYW3NE1QH0)\n",
      "\n",
      "bottom: Vanessa High-Rise Straight-Leg Jeans(01DTATGN3YQGYEPCXAD0E207TP)\n",
      "\n",
      "shoe: Juan Embossed Mules(01DVA59VHYAPT4PVX32NXW91G5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_weighted_word_embedding('01DVA59VHYAPT4PVX32NXW91G5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T06:50:02.155540Z",
     "start_time": "2021-05-11T06:31:48.980542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most similar product is pink spacedye crewneck (01EPZB9YGRNEBENQCNS24V3WWP)\n",
      "\n",
      "Following are our recommended outfit combinations:\n",
      "\n",
      "Combo 1:\n",
      "\n",
      "top: pink spacedye crewneck(01EPZB9YGRNEBENQCNS24V3WWP)\n",
      "\n",
      "bottom: embroidered gouyen shorts pink(01ED4N1V910AZ276A5HY8AYNCM)\n",
      "\n",
      "shoe: sl x two bridges hoodie in hot pink(01EHWB4Z1V5FFVA7D17DDP0ZNW)\n",
      "\n",
      "accessory: gola hoodie pink(01EF50WZEGCFG5A3Q54QWP7Q9H)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_weighted_word_embedding('pink shirt crewneck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T07:30:07.936132Z",
     "start_time": "2021-05-11T07:10:08.510754Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most similar product is toothpick high rise jeans (01E5ZX91EHF6W52B4B9GHEVJMC)\n",
      "\n",
      "Following are our recommended outfit combinations:\n",
      "\n",
      "Combo 1:\n",
      "\n",
      "top: oriana high waist straight leg velveteen jeans(01E2KX2A2EDTFS7ZPZZE3YT2A9)\n",
      "\n",
      "bottom: toothpick high rise jeans(01E5ZX91EHF6W52B4B9GHEVJMC)\n",
      "\n",
      "shoe: ★ high cut bottom – black jersey(01ET5VYBZ1VYN5ED4ZTFBD224R)\n",
      "\n",
      "accessory: the tie belted high leg high rise bottom(01EAFFATXRK3XPZRS903FQ3XDE)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_weighted_word_embedding('high rise straight jeans')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
